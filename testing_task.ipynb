{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([64, 1, 1000])\n",
      "torch.Size([64, 1])\n",
      "(tensor([[[ 16.2792,  -0.9584,   6.0445,  ...,   6.5974,   6.3012,  -2.5893]],\n",
      "\n",
      "        [[ 19.6213,  16.6265,   0.9655,  ...,   4.8606,  27.4234,  20.1950]],\n",
      "\n",
      "        [[  7.2324,  -3.7198,   4.3464,  ...,  18.3869,   9.2766,  18.9896]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  1.2743,  10.4694,  13.3491,  ..., -11.3801,  15.4625,   7.2214]],\n",
      "\n",
      "        [[ -1.1277,  14.6689,  -0.3529,  ...,   5.8412,  19.1069,  12.7201]],\n",
      "\n",
      "        [[ 20.2153,   7.3612,   9.4962,  ...,  -5.1227,   9.5780,  -2.8292]]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[16.4963],\n",
      "        [16.6755],\n",
      "        [16.9978],\n",
      "        [16.5328],\n",
      "        [16.8903],\n",
      "        [16.5693],\n",
      "        [16.4143],\n",
      "        [16.7434],\n",
      "        [17.2390],\n",
      "        [16.5309],\n",
      "        [16.8418],\n",
      "        [16.2891],\n",
      "        [16.7821],\n",
      "        [16.5837],\n",
      "        [16.7446],\n",
      "        [16.7610],\n",
      "        [16.6290],\n",
      "        [16.5438],\n",
      "        [16.6921],\n",
      "        [17.1096],\n",
      "        [16.6807],\n",
      "        [16.5888],\n",
      "        [16.6594],\n",
      "        [16.8342],\n",
      "        [16.7128],\n",
      "        [16.7817],\n",
      "        [16.2823],\n",
      "        [16.9855],\n",
      "        [16.8402],\n",
      "        [16.8037],\n",
      "        [16.6649],\n",
      "        [16.7766],\n",
      "        [16.9545],\n",
      "        [16.6616],\n",
      "        [16.8356],\n",
      "        [16.2907],\n",
      "        [16.9812],\n",
      "        [16.4916],\n",
      "        [16.7098],\n",
      "        [16.5104],\n",
      "        [16.7525],\n",
      "        [16.8407],\n",
      "        [16.5663],\n",
      "        [16.1117],\n",
      "        [16.7437],\n",
      "        [16.6893],\n",
      "        [16.5678],\n",
      "        [16.7334],\n",
      "        [16.6879],\n",
      "        [16.7916],\n",
      "        [16.8518],\n",
      "        [16.7298],\n",
      "        [17.2382],\n",
      "        [16.4077],\n",
      "        [16.8085],\n",
      "        [17.0735],\n",
      "        [16.6608],\n",
      "        [17.0586],\n",
      "        [16.9366],\n",
      "        [16.5440],\n",
      "        [17.1526],\n",
      "        [16.4835],\n",
      "        [16.6608],\n",
      "        [16.5562]], grad_fn=<ViewBackward0>))\n"
     ]
    }
   ],
   "source": [
    "#the first thing is we want to test the loss fucntion\n",
    "#we have one implementation but likely better to use the jacob schreiber version of all these things\n",
    "\n",
    "#the first thing we need to do is load in the dataloader and the bias model, we'll ignore the other hyena model right now\n",
    "\n",
    "#a simple script to test loading the bias model into pytorch\n",
    "#we have some scripts stored in bpnet lite\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/chrombpnet')\n",
    "from bpnetlite.bpnet import BPNet\n",
    "\n",
    "#now let's try loading the model using the model method\n",
    "model = BPNet.from_chrombpnet('/data/leslie/sarthak/data/chrombpnet_test/bias_model/models/k562_bias.h5')\n",
    "model\n",
    "#and this works, just had to comment out those 2 lines lol!\n",
    "#let's make a random size input\n",
    "import torch\n",
    "data = torch.rand(64,4,2114)*10\n",
    "out = model(data)\n",
    "print(len(out))\n",
    "print(out[0].shape) #the profile\n",
    "print(out[1].shape) #the counts\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m labels_profile \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels_profile\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mout\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "#now let's see how loss is calculated\n",
    "def MNLLLoss(logps, true_counts):\n",
    "\t\"\"\"A loss function based on the multinomial negative log-likelihood.\n",
    "\n",
    "\tThis loss function takes in a tensor of normalized log probabilities such\n",
    "\tthat the sum of each row is equal to 1 (e.g. from a log softmax) and\n",
    "\tan equal sized tensor of true counts and returns the probability of\n",
    "\tobserving the true counts given the predicted probabilities under a\n",
    "\tmultinomial distribution. Can accept tensors with 2 or more dimensions\n",
    "\tand averages over all except for the last axis, which is the number\n",
    "\tof categories.\n",
    "\n",
    "\tAdapted from Alex Tseng.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tlogps: torch.tensor, shape=(n, ..., L)\n",
    "\t\tA tensor with `n` examples and `L` possible categories. \n",
    "\n",
    "\ttrue_counts: torch.tensor, shape=(n, ..., L)\n",
    "\t\tA tensor with `n` examples and `L` possible categories.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tloss: float\n",
    "\t\tThe multinomial log likelihood loss of the true counts given the\n",
    "\t\tpredicted probabilities, averaged over all examples and all other\n",
    "\t\tdimensions.\n",
    "\t\"\"\"\n",
    "\n",
    "\tlog_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "\tlog_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "\tlog_prod_exp = torch.sum(true_counts * logps, dim=-1)\n",
    "\treturn -log_fact_sum + log_prod_fact - log_prod_exp\n",
    "\n",
    "labels_profile = torch.rand(64,1000)*10\n",
    "print(labels_profile.shape)\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([-78403.6719, -79383.5625, -76239.7969, -79508.6562, -77803.5547,\n",
      "        -75716.1406, -80500.1875, -76729.5859, -74137.3516, -79182.4688,\n",
      "        -76608.4453, -78685.4688, -79915.8672, -81668.2422, -79139.1016,\n",
      "        -76819.9609, -76397.5156, -78656.1250, -74702.6250, -78717.7969,\n",
      "        -78379.3594, -75467.5234, -77883.6406, -75138.4062, -74814.8438,\n",
      "        -81156.3438, -78352.8438, -80276.8984, -78168.0469, -78166.5938,\n",
      "        -75969.5625, -80774.7266, -77013.3125, -77832.3359, -75345.7188,\n",
      "        -77301.8750, -74856.0625, -78989.7734, -77662.4297, -74652.0781,\n",
      "        -74427.6406, -74569.5078, -79467.2578, -77182.9531, -79619.4531,\n",
      "        -78124.8516, -77646.7422, -74681.9688, -75639.5781, -77759.8828,\n",
      "        -75653.9062, -78317.3438, -76519.2500, -78632.3750, -77222.9688,\n",
      "        -76150.3516, -77275.3984, -75491.6484, -76596.8594, -73243.0469,\n",
      "        -73426.8359, -78307.6719, -74891.5000, -77744.3828],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = MNLLLoss(out[0].squeeze(), labels_profile)\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "inhomogeneous total_count is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_prob\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 27\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m \u001b[43mcbpnet_multinomial_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_profile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss2)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mcbpnet_multinomial_nll\u001b[0;34m(logits, true_counts, len_batch, ignore_index, mask, weight, pos_weight)\u001b[0m\n\u001b[1;32m     14\u001b[0m counts_per_example \u001b[38;5;241m=\u001b[39m true_counts\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create the Multinomial distribution with the given logits\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcounts_per_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the negative log probability of the true counts\u001b[39;00m\n\u001b[1;32m     20\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(true_counts)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/distributions/multinomial.py:61\u001b[0m, in \u001b[0;36mMultinomial.__init__\u001b[0;34m(self, total_count, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, total_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(total_count, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minhomogeneous total_count is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_count \u001b[38;5;241m=\u001b[39m total_count\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_categorical \u001b[38;5;241m=\u001b[39m Categorical(probs\u001b[38;5;241m=\u001b[39mprobs, logits\u001b[38;5;241m=\u001b[39mlogits)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: inhomogeneous total_count is not supported"
     ]
    }
   ],
   "source": [
    "def cbpnet_multinomial_nll(logits,true_counts, len_batch=None, ignore_index=-100, mask = True, weight = .5, pos_weight = .04):\n",
    "    \"\"\"\n",
    "    Compute the multinomial negative log-likelihood in PyTorch.\n",
    "\n",
    "    Args:\n",
    "      true_counts: Observed count values (tensor of shape [batch_size, num_classes])\n",
    "      logits: Predicted logit values (tensor of shape [batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    #check if the input is a tuple\n",
    "    if isinstance(logits, tuple): #for wandb tracking, it inputs logits here, since then it's a tuple\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # Calculate the total counts per example\n",
    "    counts_per_example = true_counts.sum(dim=-1)\n",
    "\n",
    "    # Create the Multinomial distribution with the given logits\n",
    "    dist = torch.distributions.Multinomial(total_count=counts_per_example, logits=logits)\n",
    "\n",
    "    # Calculate the negative log probability of the true counts\n",
    "    log_prob = dist.log_prob(true_counts)\n",
    "\n",
    "    # Calculate the mean NLL loss over the batch\n",
    "    loss = -log_prob.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "loss2 = cbpnet_multinomial_nll(out[0].squeeze(), labels_profile)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4, 2114)\n",
      "(64, 1000)\n"
     ]
    }
   ],
   "source": [
    "#yeah this is not going to work well, let's just say no to this...\n",
    "#but if we use the schreiber one, do we need to do log first?\n",
    "#let's just give the same input to both and see what the output is\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/chrombpnet')\n",
    "from bpnetlite.bpnet import BPNet\n",
    "\n",
    "model = BPNet.from_chrombpnet('/data/leslie/sarthak/data/chrombpnet_test/bias_model/models/k562_bias.h5')\n",
    "import torch\n",
    "import numpy as np\n",
    "array1 = np.array(range(64*4*2114)).reshape(64,4,2114) / 1000\n",
    "print(array1.shape)\n",
    "array2 = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "print(array2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 698.4480, 1170.9492, 1439.0846, 1635.6626, 1795.2100, 1932.0447,\n",
      "        2053.5070, 2163.8679, 2265.8329, 2361.2251, 2451.3312, 2537.0936,\n",
      "        2619.2231, 2698.2691, 2774.6652, 2848.7593, 2920.8352, 2991.1271,\n",
      "        3059.8307, 3127.1112, 3193.1092, 3257.9456, 3321.7249, 3384.5379,\n",
      "        3446.4645, 3507.5749, 3567.9314, 3627.5892, 3686.5979, 3745.0019,\n",
      "        3802.8411, 3860.1517, 3916.9666, 3973.3155, 4029.2257, 4084.7220,\n",
      "        4139.8273, 4194.5626, 4248.9472, 4302.9991, 4356.7347, 4410.1694,\n",
      "        4463.3175, 4516.1922, 4568.8058, 4621.1699, 4673.2953, 4725.1921,\n",
      "        4776.8697, 4828.3372, 4879.6027, 4930.6743, 4981.5594, 5032.2648,\n",
      "        5082.7974, 5133.1633, 5183.3684, 5233.4184, 5283.3187, 5333.0741,\n",
      "        5382.6897, 5432.1699, 5481.5190, 5530.7413], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#let's now calculate the loss\n",
    "def cbpnet_multinomial_nll(logits,true_counts, len_batch=None, ignore_index=-100, mask = True, weight = .5, pos_weight = .04):\n",
    "\n",
    "    \"\"\"A loss function based on the multinomial negative log-likelihood.\n",
    "    modified by me to include things like doing thE log softmax\n",
    "\n",
    "    This loss function takes in a tensor of normalized log probabilities such\n",
    "    that the sum of each row is equal to 1 (e.g. from a log softmax) and\n",
    "    an equal sized tensor of true counts and returns the probability of\n",
    "    observing the true counts given the predicted probabilities under a\n",
    "    multinomial distribution. Can accept tensors with 2 or more dimensions\n",
    "    and averages over all except for the last axis, which is the number\n",
    "    of categories.\n",
    "\n",
    "    Adapted from Alex Tseng.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logps: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories. \n",
    "\n",
    "    true_counts: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        The multinomial log likelihood loss of the true counts given the\n",
    "        predicted probabilities, averaged over all examples and all other\n",
    "        dimensions.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple): #for wandb tracking, it inputs logits here, since then it's a tuple\n",
    "        logits = logits[0]\n",
    "    logps = torch.log_softmax(logits, dim=-1)\n",
    "    log_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "    log_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "    log_prod_exp = torch.sum(true_counts * logps, dim=-1)\n",
    "    return -log_fact_sum + log_prod_fact - log_prod_exp\n",
    "\n",
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_logits = torch.tensor(fake_logits)\n",
    "fake_labels = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_labels = torch.tensor(fake_labels)\n",
    "\n",
    "loss = cbpnet_multinomial_nll(fake_logits, fake_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3744.9462, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(loss.mean()) #so we do indeed need to average it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0221e+03, -1.1498e+04, -2.2178e+04, -3.4929e+04, -4.9718e+04,\n",
      "        -6.6529e+04, -8.5356e+04, -1.0619e+05, -1.2904e+05, -1.5389e+05,\n",
      "        -1.8075e+05, -2.0961e+05, -2.4048e+05, -2.7335e+05, -3.0822e+05,\n",
      "        -3.4509e+05, -3.8397e+05, -4.2485e+05, -4.6773e+05, -5.1261e+05,\n",
      "        -5.5949e+05, -6.0837e+05, -6.5926e+05, -7.1214e+05, -7.6703e+05,\n",
      "        -8.2391e+05, -8.8280e+05, -9.4369e+05, -1.0066e+06, -1.0715e+06,\n",
      "        -1.1384e+06, -1.2073e+06, -1.2781e+06, -1.3510e+06, -1.4259e+06,\n",
      "        -1.5028e+06, -1.5817e+06, -1.6626e+06, -1.7455e+06, -1.8304e+06,\n",
      "        -1.9173e+06, -2.0062e+06, -2.0971e+06, -2.1900e+06, -2.2849e+06,\n",
      "        -2.3818e+06, -2.4807e+06, -2.5816e+06, -2.6845e+06, -2.7893e+06,\n",
      "        -2.8962e+06, -3.0051e+06, -3.1160e+06, -3.2289e+06, -3.3438e+06,\n",
      "        -3.4607e+06, -3.5796e+06, -3.7005e+06, -3.8234e+06, -3.9483e+06,\n",
      "        -4.0752e+06, -4.2041e+06, -4.3350e+06, -4.4679e+06],\n",
      "       dtype=torch.float64)\n",
      "tensor(-1583840.1424, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#what if we don'to do the log softmax?\n",
    "\n",
    "#let's now calculate the loss\n",
    "def cbpnet_multinomial_nll(logits,true_counts, len_batch=None, ignore_index=-100, mask = True, weight = .5, pos_weight = .04):\n",
    "\n",
    "    \"\"\"A loss function based on the multinomial negative log-likelihood.\n",
    "    modified by me to include things like doing thE log softmax\n",
    "\n",
    "    This loss function takes in a tensor of normalized log probabilities such\n",
    "    that the sum of each row is equal to 1 (e.g. from a log softmax) and\n",
    "    an equal sized tensor of true counts and returns the probability of\n",
    "    observing the true counts given the predicted probabilities under a\n",
    "    multinomial distribution. Can accept tensors with 2 or more dimensions\n",
    "    and averages over all except for the last axis, which is the number\n",
    "    of categories.\n",
    "\n",
    "    Adapted from Alex Tseng.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logps: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories. \n",
    "\n",
    "    true_counts: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        The multinomial log likelihood loss of the true counts given the\n",
    "        predicted probabilities, averaged over all examples and all other\n",
    "        dimensions.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple): #for wandb tracking, it inputs logits here, since then it's a tuple\n",
    "        logits = logits[0]\n",
    "    logps = logits\n",
    "    log_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "    log_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "    log_prod_exp = torch.sum(true_counts * logps, dim=-1)\n",
    "    return -log_fact_sum + log_prod_fact - log_prod_exp\n",
    "\n",
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_logits = torch.tensor(fake_logits)\n",
    "fake_labels = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_labels = torch.tensor(fake_labels)\n",
    "\n",
    "loss = cbpnet_multinomial_nll(fake_logits, fake_labels)\n",
    "print(loss)\n",
    "print(loss.mean())\n",
    "#we get a massive number now... it claims we need the log softmax but I don't see it in the chrombpnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 698.4480, 1170.9492, 1439.0846, 1635.6626, 1795.2100, 1932.0447,\n",
      "        2053.5070, 2163.8679, 2265.8329, 2361.2251, 2451.3312, 2537.0936,\n",
      "        2619.2231, 2698.2691, 2774.6652, 2848.7593, 2920.8352, 2991.1271,\n",
      "        3059.8307, 3127.1112, 3193.1092, 3257.9456, 3321.7249, 3384.5379,\n",
      "        3446.4645, 3507.5749, 3567.9314, 3627.5892, 3686.5979, 3745.0019,\n",
      "        3802.8411, 3860.1517, 3916.9666, 3973.3155, 4029.2257, 4084.7220,\n",
      "        4139.8273, 4194.5626, 4248.9472, 4302.9991, 4356.7347, 4410.1694,\n",
      "        4463.3175, 4516.1922, 4568.8058, 4621.1699, 4673.2953, 4725.1921,\n",
      "        4776.8697, 4828.3372, 4879.6027, 4930.6743, 4981.5594, 5032.2648,\n",
      "        5082.7974, 5133.1633, 5183.3684, 5233.4184, 5283.3187, 5333.0741,\n",
      "        5382.6897, 5432.1699, 5481.5190, 5530.7413], dtype=torch.float64)\n",
      "tensor(3744.9462, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#and certainly there isn't in the tensorflow, otherwise we'd be doing it twice\n",
    "\n",
    "#what if we don'to do the log softmax?\n",
    "\n",
    "#let's now calculate the loss\n",
    "def cbpnet_multinomial_nll(logits,true_counts, len_batch=None, ignore_index=-100, mask = True, weight = .5, pos_weight = .04):\n",
    "\n",
    "    \"\"\"A loss function based on the multinomial negative log-likelihood.\n",
    "    modified by me to include things like doing thE log softmax\n",
    "\n",
    "    This loss function takes in a tensor of normalized log probabilities such\n",
    "    that the sum of each row is equal to 1 (e.g. from a log softmax) and\n",
    "    an equal sized tensor of true counts and returns the probability of\n",
    "    observing the true counts given the predicted probabilities under a\n",
    "    multinomial distribution. Can accept tensors with 2 or more dimensions\n",
    "    and averages over all except for the last axis, which is the number\n",
    "    of categories.\n",
    "\n",
    "    Adapted from Alex Tseng.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logps: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories. \n",
    "\n",
    "    true_counts: torch.tensor, shape=(n, ..., L)\n",
    "        A tensor with `n` examples and `L` possible categories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        The multinomial log likelihood loss of the true counts given the\n",
    "        predicted probabilities, averaged over all examples and all other\n",
    "        dimensions.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple): #for wandb tracking, it inputs logits here, since then it's a tuple\n",
    "        logits = logits[0]\n",
    "    logps = torch.log_softmax(logits, dim=-1)\n",
    "    logps = torch.log_softmax(logps, dim=-1)\n",
    "    log_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "    log_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "    log_prod_exp = torch.sum(true_counts * logps, dim=-1)\n",
    "    return -log_fact_sum + log_prod_fact - log_prod_exp\n",
    "\n",
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_logits = torch.tensor(fake_logits)\n",
    "fake_logits = torch.log_softmax(fake_logits, dim=-1)\n",
    "fake_labels = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "fake_labels = torch.tensor(fake_labels)\n",
    "\n",
    "loss = cbpnet_multinomial_nll(fake_logits, fake_labels)\n",
    "print(loss)\n",
    "print(loss.mean())\n",
    "#wait this tells us that tf might be doing log softmax in the loss, it isn't in the model but they could still do it, this would make a lot of sense!\n",
    "#this is because applying log softmax after it's already been done once leaves it unchanged!!\n",
    "\n",
    "#so let's see if in tensorflow they apply log softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:07:36.617668: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 15:07:41.997529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7410 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-06-17 15:07:41.999094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7410 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-06-17 15:07:42.000409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 7410 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1\n",
      "2024-06-17 15:07:42.000999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 7410 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.cast(tf.shape(true_counts)[0], dtype=tf.float64))\n",
    "\n",
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "#do log softmax of fake_logits\n",
    "fake_logits = tf.nn.log_softmax(fake_logits, axis=-1)\n",
    "fake_labels = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "#now make fake_labels a tf tensor\n",
    "fake_labels = tf.convert_to_tensor(fake_labels)\n",
    "\n",
    "loss = multinomial_nll(fake_labels, fake_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3744.9462380055356>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# fake_logits.shape #yeah this looks right!\n",
    "\n",
    "loss = multinomial_nll(fake_labels, fake_logits) #this immediately crashes the kernel...\n",
    "#i think it's accessing tensorflow that's the issue where it tries to accessa gpu but can't...\n",
    "#also if we look at the model, we see that \n",
    "\n",
    "'''\n",
    "This is the output that I get\n",
    "\n",
    "<tf.Tensor: shape=(), dtype=float64, numpy=3744.9462380055356>\n",
    "\n",
    "Exactly the same as jacob schreiber's so we know we are doing the loss correctly!!!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what happens if we don't apply log softmax\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.cast(tf.shape(true_counts)[0], dtype=tf.float64))\n",
    "\n",
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "#do log softmax of fake_logits\n",
    "# fake_logits = tf.nn.log_softmax(fake_logits, axis=-1)\n",
    "fake_labels = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "#now make fake_labels a tf tensor\n",
    "# fake_labels = tf.convert_to_tensor(fake_labels)\n",
    "\n",
    "loss = multinomial_nll(fake_labels, fake_logits)\n",
    "print(loss)\n",
    "\n",
    "#when we run it we get\n",
    "#tf.Tensor(3744.9462380055356, shape=(), dtype=float64)...\n",
    "#oh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000e+00 1.0000e-03 2.0000e-03 ... 9.9700e-01 9.9800e-01 9.9900e-01]\n",
      " [1.0000e+00 1.0010e+00 1.0020e+00 ... 1.9970e+00 1.9980e+00 1.9990e+00]\n",
      " [2.0000e+00 2.0010e+00 2.0020e+00 ... 2.9970e+00 2.9980e+00 2.9990e+00]\n",
      " ...\n",
      " [6.1000e+01 6.1001e+01 6.1002e+01 ... 6.1997e+01 6.1998e+01 6.1999e+01]\n",
      " [6.2000e+01 6.2001e+01 6.2002e+01 ... 6.2997e+01 6.2998e+01 6.2999e+01]\n",
      " [6.3000e+01 6.3001e+01 6.3002e+01 ... 6.3997e+01 6.3998e+01 6.3999e+01]]\n",
      "tensor([[-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        ...,\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "fake_logits = np.array(range(64*1000)).reshape(64,1000) / 1000\n",
    "logmaxed = torch.log_softmax(torch.tensor(fake_logits), dim=-1)\n",
    "print(fake_logits)\n",
    "print(logmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        ...,\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496],\n",
      "        [-7.4486, -7.4476, -7.4466,  ..., -6.4516, -6.4506, -6.4496]],\n",
      "       dtype=torch.float64)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#now do it again\n",
    "logmaxed2 = torch.log_softmax(logmaxed, dim=-1)\n",
    "print(logmaxed2)\n",
    "print(np.allclose(logmaxed, logmaxed2)) #this is true, so we can do log softmax twice and it's the same as doing it once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor(1.0000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Logo softmax makes it such that the sum of the exponentials of any row are 1\n",
    "print(torch.exp(logmaxed[0]).shape)\n",
    "print(torch.sum(torch.exp(logmaxed[0]))) #this is 1 as we expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but if the sum of exponentials is 1, then leaving it as such will keep it as 1!\n",
    "\n",
    "#however, in the torch function, they don't do the log softmax but they use probabilities which basically do that!\n",
    "\n",
    "#I'm not too sure how it's calculated, but we know it's correct so let's use it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
