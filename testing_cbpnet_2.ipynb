{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a lot of issues, now we solve this issue which is that it doesn't seem to be ok with this larger input, let's modify this to work with a smaller number of filters and see if it works\n",
    "import numpy as np ;\n",
    "from tensorflow.keras.backend import int_shape\n",
    "from tensorflow.keras.layers import Input, Cropping1D, add, Conv1D, GlobalAvgPool1D, Dense, Add, Concatenate, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from chrombpnet.training.utils.losses import multinomial_nll\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os \n",
    "def bpnet_model(filters, n_dil_layers, sequence_len, out_pred_len):\n",
    "\n",
    "    conv1_kernel_size=21\n",
    "    profile_kernel_size=75\n",
    "    num_tasks=1 # not using multi tasking\n",
    "\n",
    "    #define inputs\n",
    "    inp = Input(shape=(sequence_len, 4),name='sequence')    \n",
    "\n",
    "    # first convolution without dilation\n",
    "    x = Conv1D(filters,\n",
    "                kernel_size=conv1_kernel_size,\n",
    "                padding='valid', \n",
    "                activation='relu',\n",
    "                name='wo_bias_bpnet_1st_conv')(inp)\n",
    "\n",
    "    layer_names = [str(i) for i in range(1,n_dil_layers+1)]\n",
    "    for i in range(1, n_dil_layers + 1):\n",
    "        # dilated convolution\n",
    "        conv_layer_name = 'wo_bias_bpnet_{}conv'.format(layer_names[i-1])\n",
    "        conv_x = Conv1D(filters, \n",
    "                        kernel_size=3, \n",
    "                        padding='valid',\n",
    "                        activation='relu', \n",
    "                        dilation_rate=2**i,\n",
    "                        name=conv_layer_name)(x)\n",
    "\n",
    "        x_len = int_shape(x)[1]\n",
    "        conv_x_len = int_shape(conv_x)[1]\n",
    "        assert((x_len - conv_x_len) % 2 == 0) # Necessary for symmetric cropping\n",
    "\n",
    "        x = Cropping1D((x_len - conv_x_len) // 2, name=\"wo_bias_bpnet_{}crop\".format(layer_names[i-1]))(x)\n",
    "        x = add([conv_x, x])\n",
    "\n",
    "    # Branch 1. Profile prediction\n",
    "    # Step 1.1 - 1D convolution with a very large kernel\n",
    "    prof_out_precrop = Conv1D(filters=num_tasks,\n",
    "                        kernel_size=profile_kernel_size,\n",
    "                        padding='valid',\n",
    "                        name='wo_bias_bpnet_prof_out_precrop')(x)\n",
    "\n",
    "    # Step 1.2 - Crop to match size of the required output size\n",
    "    cropsize = int(int_shape(prof_out_precrop)[1]/2)-int(out_pred_len/2)\n",
    "    assert cropsize>=0\n",
    "    assert (int_shape(prof_out_precrop)[1] % 2 == 0) # Necessary for symmetric cropping\n",
    "\n",
    "    prof = Cropping1D(cropsize,\n",
    "                name='wo_bias_bpnet_logitt_before_flatten')(prof_out_precrop)\n",
    "    \n",
    "    profile_out = Flatten(name=\"wo_bias_bpnet_logits_profile_predictions\")(prof)\n",
    "\n",
    "    # Branch 2. Counts prediction\n",
    "    # Step 2.1 - Global average pooling along the \"length\", the result\n",
    "    #            size is same as \"filters\" parameter to the BPNet function\n",
    "    gap_combined_conv = GlobalAvgPool1D(name='gap')(x) # acronym - gapcc\n",
    "\n",
    "    # Step 2.3 Dense layer to predict final counts\n",
    "    count_out = Dense(num_tasks, name=\"wo_bias_bpnet_logcount_predictions\")(gap_combined_conv)\n",
    "\n",
    "    # instantiate keras Model with inputs and outputs\n",
    "    model=Model(inputs=[inp],outputs=[profile_out, count_out], name=\"model_wo_bias\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:37:13.146525: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 00:37:18.387749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10438 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-06-14 00:37:18.389352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10438 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-06-14 00:37:18.391819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10254 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1\n",
      "2024-06-14 00:37:18.393058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10288 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in wo_bias_bpnet_8conv. Consider increasing the input size. Received input shape [None, 496, 512] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbpnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mbpnet_model\u001b[0;34m(filters, n_dil_layers, sequence_len, out_pred_len)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_dil_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# dilated convolution\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     conv_layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo_bias_bpnet_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(layer_names[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m     conv_x \u001b[38;5;241m=\u001b[39m \u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     x_len \u001b[38;5;241m=\u001b[39m int_shape(x)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     39\u001b[0m     conv_x_len \u001b[38;5;241m=\u001b[39m int_shape(conv_x)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/chrombpnet/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/chrombpnet/lib/python3.10/site-packages/keras/layers/convolutional.py:304\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[1;32m    300\u001b[0m         input_shape[:batch_rank] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    306\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    308\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    309\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    310\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in wo_bias_bpnet_8conv. Consider increasing the input size. Received input shape [None, 496, 512] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "model = bpnet_model(512, 8, 1024, 800)\n",
    "#this is the exact error we're running into!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bpnet_model(512, 5, 1024, 800)\n",
    "#seems we have to limit the number of dilated layers to 5, let's see if this works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
